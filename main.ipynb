{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def load_dataset():\n",
    "    dataset = read_data('actitracker_raw.txt')\n",
    "    dataset.dropna(axis=0, how='any', inplace= True)\n",
    "    dataset.drop_duplicates(['user_id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis'], keep= 'first', inplace= True)\n",
    "    dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "    dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "    dataset['z-axis'] = pd.to_numeric(dataset['z-axis'].str.replace(';',''))\n",
    "    dataset['z-axis'] = feature_normalize(dataset['z-axis'])\n",
    "    return dataset    \n",
    "\n",
    "# read data\n",
    "def read_data(file_path):\n",
    "    column_names = ['user_id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names)\n",
    "    return data\n",
    "\n",
    "# normalize data\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "# slide windows\n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "# segment signal\n",
    "def segment_signal(data,window_size = 128):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"activity\"][start:end])[0][0])\n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model and evaluation\n",
    "def evaluate_model(train_x, train_y, test_x, test_y):\n",
    "\n",
    "    verbose, epochs, batch_size = 0, 15, 32\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    model = Sequential()\n",
    "\n",
    "    learning_rate = 0.001 # With the default value of learning rate\n",
    "    l2_rate = 1e-4\n",
    "    input_shape = Input(shape=(n_timesteps,n_features))\n",
    "    pool_size = 2\n",
    "    \n",
    "    x = Conv1D(32, kernel_size = 24, \n",
    "            activation = \"relu\", \n",
    "            padding = \"valid\", \n",
    "            kernel_regularizer = l2(l2_rate))(input_shape)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    \n",
    "    x = Conv1D(64, kernel_size = 16, \n",
    "            activation = \"relu\", \n",
    "            padding = \"valid\", \n",
    "            kernel_regularizer = l2(l2_rate))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    \n",
    "    x = Conv1D(96, kernel_size = 8, \n",
    "            activation = \"relu\", \n",
    "            padding = \"valid\", \n",
    "            kernel_regularizer = l2(l2_rate))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    \n",
    "    x = Conv1D(128, kernel_size = 4, \n",
    "            activation = \"relu\", \n",
    "            padding = \"valid\", \n",
    "            kernel_regularizer = l2(l2_rate),\n",
    "            name=\"encoder\")(x)\n",
    "    x = Flatten()(x) \n",
    "    output = Dense(n_outputs, activation = \"softmax\")(x)\n",
    "    \n",
    "    model = Model(input_shape, output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=[AUC()])\n",
    "    \n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose, \n",
    "              validation_split=0.33, callbacks=[es])\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # return results\n",
    "    return model, accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats, train_x, train_y, test_x, test_y):\n",
    "    \n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        model, score = evaluate_model(train_x, train_y, test_x, test_y)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return model, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = read_data('data/dataset.txt')\n",
    "users = [17,35, 20, 6, 15, 28, 5, 10, 18, 11, 34, 1, 24, 26, 12, 32, 31, 13]\n",
    "window_size = 128\n",
    "n_features = 3\n",
    "users_all = []\n",
    "scores_all = []\n",
    "\n",
    "# train the CNN by each user\n",
    "for user in users:\n",
    "    print(\"------------------------\", user, \"------------------------\")\n",
    "    \n",
    "    dataset = load_dataset()\n",
    "    dataset = dataset[dataset[\"user_id\"] == user]\n",
    "    print(dataset.shape)\n",
    "    \n",
    "    segments, labels = segment_signal(dataset)\n",
    "    labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "    reshaped_segments = segments.reshape(len(segments), window_size , n_features)\n",
    "\n",
    "    # shuffle and split training and test sets\n",
    "    random_state = 42\n",
    "    train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "    train_x = reshaped_segments[train_test_split]\n",
    "    train_y = labels[train_test_split]\n",
    "    test_x = reshaped_segments[~train_test_split]\n",
    "    test_y = labels[~train_test_split]\n",
    "\n",
    "    model, scores = run_experiment(repeats=10, train_x=train_x, train_y=train_y, test_x=test_x, test_y=test_y)\n",
    "    \n",
    "    for score in scores:\n",
    "        users_all.append(user)\n",
    "        scores_all.append(score)\n",
    "\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export accuracies\n",
    "data = {'user_id':users_all,\n",
    "        'accuracy':scores_all,\n",
    "       }\n",
    "df = pd.DataFrame(data=data)\n",
    "df.to_csv('accuracies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export methods\n",
    "method = {'uid':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'user_id':[users],\n",
    "        'method':[\"Proposed\", \"Proposed\", \"Proposed\", \"Proposed\", \"Proposed\", \"Proposed\", \"Proposed\", \"Proposed\", \"Proposed\", \"Proposed\",\n",
    "                 \"Baseline\", \"Baseline\", \"Baseline\", \"Baseline\", \"Baseline\", \"Baseline\", \"Baseline\", \"Baseline\", \"Baseline\", \"Baseline\"],\n",
    "       }\n",
    "df_method = pd.DataFrame(data=method)\n",
    "df_method.to_csv('method.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
